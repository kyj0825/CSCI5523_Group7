{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amended-packing",
   "metadata": {},
   "source": [
    "# CSCI5523 Group7 Project on Gendered Wording in IT Job Posting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-speaking",
   "metadata": {},
   "source": [
    "### Step 1. Data Import and Preprocessing for Topic Modeling and Linguistic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\kwonr\\Dropbox\\JobAd_Gender\\Analysis')\n",
    "\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "##DB connection - connect()\n",
    "\n",
    "posting = pymysql.connect( \n",
    "    user='root', \n",
    "    passwd='0825', \n",
    "    host='127.0.0.1', \n",
    "    db='job_posting', \n",
    "    charset='utf8',\n",
    "    use_unicode=True\n",
    ")\n",
    "\n",
    "## cursor - cursor()\n",
    "cursor = posting.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "## Table & Column Selection - SELECT\n",
    "sql = \"SELECT * FROM `_sb2j_clusr_cl_job`;\"\n",
    "cursor.execute(sql)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "## Pandas Dataframe conversion\n",
    "pd.set_option('display.max_columns', None)\n",
    "df= pd.DataFrame(result)\n",
    "\n",
    "# Understanding DataType\n",
    "pd.set_option('display.max_rows', 100)\n",
    "df.dtypes\n",
    "\n",
    "# Make an index column for reference & later merge\n",
    "df['index_col'] = df.index\n",
    "df\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "import tarfile\n",
    "import chardet\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "p = '[a-zA-Z0-9]+'\n",
    "stopwords={\n",
    "    'max>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax':1,\n",
    "    'edu':1,\n",
    "    'subject':1,\n",
    "    'com':1,\n",
    "    'r<g':1,\n",
    "    '_?w':1,\n",
    "    'isc':1,\n",
    "    'cx^':1,\n",
    "    'usr':1,\n",
    "    'uga':1,\n",
    "    'sam':1,\n",
    "    'mhz':1,\n",
    "    'b8f':1,\n",
    "    '34u':1,\n",
    "    'pl+':1,\n",
    "    '1993apr20':1,\n",
    "    '1993apr15':1,\n",
    "    'xterm':1,\n",
    "    'utexas':1,\n",
    "    'x11r5':1,\n",
    "    'o+r':1,\n",
    "    'iastate':1,\n",
    "    'udel':1,\n",
    "    'uchicago':1,\n",
    "    '1993apr21':1,\n",
    "    'uxa':1,\n",
    "    'argic':1,\n",
    "    'optilink':1,\n",
    "    'imho':1,\n",
    "    'umich':1,\n",
    "    'openwindows':1,\n",
    "    '1993apr19':1,\n",
    "    '1993apr22':1,\n",
    "\n",
    "}\n",
    "vocabulary = {}\n",
    "freq_threshold = 50\n",
    "word_len_threshold = 2\n",
    "ps=PorterStemmer()\n",
    "\n",
    "\n",
    "def load_stopwords():\n",
    "    # url: https://github.com/igorbrigadir/stopwords/blob/master/en/gensim.txt\n",
    "    stopword_file = 'gensim_stopwords.txt'\n",
    "    with open(stopword_file, mode='r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            word = line.strip()\n",
    "            stopwords[word] = 1\n",
    "    # print(stopwords[:50])\n",
    "\n",
    "\n",
    "def load_vocab(path):\n",
    "    vocab = []\n",
    "    vocab_dict = {}\n",
    "    with open(path, mode='r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            seg = line.strip().split()\n",
    "            vocab.append(seg[0])\n",
    "            vocab_dict[seg[0]] = len(vocab) # dict count from 1\n",
    "    return vocab, vocab_dict\n",
    "\n",
    "\n",
    "def clean_words(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        word = word.strip('_\\[\\]\\'\\\".,()*! #@~`\\\\%^&;:/-+=“”‘’<>{}|?$^').replace('isn\\'t', '').replace('\\'s', '').replace('\\'re', '').replace('\\'t', '').replace('\\'ll', '').replace('\\'m', '').replace('\\'am', '').replace('\\'ve', '').replace('\\'d', '')\n",
    "        segs = re.split('[()@.\\-/#\\\\\\\\\"`\\[\\]]', word)\n",
    "        new_word = []\n",
    "        for s in segs:\n",
    "            seg=s\n",
    "            # seg = ps.stem(seg)\n",
    "            if seg not in stopwords and seg and len(seg) > word_len_threshold:\n",
    "                new_word.append(seg)\n",
    "        # word = ' '.join(new_word)\n",
    "        # if word and len(word) > word_len_threshold:\n",
    "        #     if word not in stopwords:\n",
    "        #           new_words.append(word)\n",
    "        new_words.extend(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def read_zip(path):\n",
    "    data=[]\n",
    "    if os.path.exists(path):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for _file in files:\n",
    "                # print(os.path.join(root, _file))\n",
    "                content = []\n",
    "                with open(os.path.join(root, _file), mode='rb') as reader:\n",
    "                    content = reader.read()\n",
    "                encode_info = chardet.detect(content)['encoding']\n",
    "                if not encode_info:\n",
    "                    encode_info = 'utf-8'\n",
    "                content = content.decode(encoding=encode_info)\n",
    "                content = content.split()\n",
    "                data.append(' '.join(content))\n",
    "    return data\n",
    "\n",
    "def read_file(path):\n",
    "    data = []\n",
    "    with open(path, mode='r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            line = line.strip()\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "def parse_sent(line):\n",
    "    delim = '[,.?!]'\n",
    "    sents = []\n",
    "    segs = re.split(delim, line)\n",
    "    for seg in segs:\n",
    "        if seg not in delim and len(seg)>5:\n",
    "            sents.append(seg)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def load_files_nvdm(path, vocab_dict,ifSplit=False):\n",
    "    labels = {}\n",
    "    data = []\n",
    "    label_count=[]\n",
    "    count = 0\n",
    "    if os.path.exists(path):\n",
    "        data = read_file(path)\n",
    "    new_docs = []\n",
    "    new_doc_sents = []\n",
    "    for doc in data:\n",
    "        new_doc = {}\n",
    "        segs = doc.split()\n",
    "        line = segs[-2::-1]\n",
    "        label = segs[-1]\n",
    "        label_count.append(label)\n",
    "        words = line\n",
    "        words = clean_words(words)\n",
    "        for word in words:\n",
    "            if word in vocab_dict:\n",
    "                count += 1\n",
    "                word_id = vocab_dict[word]\n",
    "                if word_id in new_doc:\n",
    "                    new_doc[word_id] += 1\n",
    "                else:\n",
    "                    new_doc[word_id] = 1\n",
    "\n",
    "        # print(new_doc)\n",
    "        if new_doc:\n",
    "            new_docs.append(' '.join(['{}:{}'.format(item[0], item[1]) for item in new_doc.items()]))\n",
    "            # print(new_docs[-1])\n",
    "\n",
    "    idx = list(range(1, len(data)+1))\n",
    "    # test_idx = list(np.random.choice(idx, 7531, replace=False))\n",
    "    test_idx = []\n",
    "    print('avg length:', count/len(data),count,len(data))\n",
    "    doc_test=[]\n",
    "    doc_train=[]\n",
    "    for k, doc in enumerate(new_docs):\n",
    "        line_doc = label_count[k]+' '+doc\n",
    "        if k in test_idx:\n",
    "            doc_test.append(line_doc)\n",
    "        else:\n",
    "            doc_train.append(line_doc)\n",
    "    if doc_test:\n",
    "        write_file('test.feat', doc_test)\n",
    "    if doc_train:\n",
    "        write_file('train.feat', doc_train)\n",
    "\n",
    "    # with open('20news', mode='w', encoding='utf-8') as writer:\n",
    "    #     writer.write('\\n'.join(new_docs))\n",
    "    # return data\n",
    "\n",
    "def load_files_prodlda(path, vocab_dict):\n",
    "    data = []\n",
    "    if os.path.exists(path):\n",
    "        data = read_file(path)\n",
    "    new_docs = []\n",
    "    for doc in data:\n",
    "        segs = doc.split()\n",
    "        line = segs[-2::-1]\n",
    "        words = line\n",
    "        words = clean_words(words)\n",
    "        if words:\n",
    "            new_docs.append(words)\n",
    "    process_prolda(new_docs, vocab_dict, path+'.npy')\n",
    "\n",
    "def write_file(path, data):\n",
    "    with open(path, mode='w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(data))\n",
    "\n",
    "\n",
    "def produce_files(paths):\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        data = read_file(path)\n",
    "        new_data = []\n",
    "        for line in data:\n",
    "            segs = line.split()\n",
    "            doc = []\n",
    "            for seg in segs[1:]:\n",
    "                word_id, freq = seg.split(':')\n",
    "                doc.extend([int(word_id)]*int(freq))\n",
    "            doc = np.array(doc)\n",
    "            new_data.append(doc)\n",
    "        new_data = np.array(new_data)\n",
    "        np.save(path, new_data)\n",
    "\n",
    "def process_prolda(data, vocab, outputfile):\n",
    "    new_docs = []\n",
    "    for doc in data:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            if word in vocab:\n",
    "                new_doc.append(vocab[word])\n",
    "        new_doc = np.array(new_doc)\n",
    "        new_docs.append(new_doc)\n",
    "    new_docs = np.array(new_docs)\n",
    "    print(new_docs.shape)\n",
    "    np.save(outputfile, new_docs)\n",
    "\n",
    "def clean_vocab(paths):\n",
    "    label = []\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        data.extend(read_file(path))\n",
    "    for line in data:\n",
    "        seg = line.split()\n",
    "        text = seg[-2::-1]\n",
    "        # text = line\n",
    "        # text = text.strip()\n",
    "        words = text\n",
    "        words = clean_words(words)\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                vocabulary[word] = vocabulary[word] + 1\n",
    "            else:\n",
    "                vocabulary[word] = 1\n",
    "\n",
    "    sorted_vocab = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "    # sorted_vocab = [item for item in sorted_vocab if item[1] > freq_threshold]\n",
    "    sorted_vocab = sorted_vocab[-1:-5001:-1]\n",
    "    #\n",
    "    # write vocablary\n",
    "\n",
    "    texts = ['{} {}'.format(item[0], item[1]) for item in sorted_vocab]\n",
    "    with open('vocab.new', mode='w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(texts))\n",
    "\n",
    "\n",
    "def create_corpus(paths, vocab_url):\n",
    "    vocab, v_dict = load_vocab(vocab_url)\n",
    "    data=[]\n",
    "    for path in paths:\n",
    "        data.extend(read_file(path))\n",
    "    new_data = []\n",
    "    for line in data:\n",
    "        doc = []\n",
    "        segs = line.strip().split()\n",
    "        for seg in segs[1:]:\n",
    "            word_id, freq = seg.split(':')\n",
    "            tmp = [vocab[int(word_id)-1]] *int(freq)\n",
    "            # print(tmp)\n",
    "            doc.extend(tmp)\n",
    "        doc = ' '.join(doc)\n",
    "        new_data.append(doc)\n",
    "    with open('Snippets', mode='w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(new_data))\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    load_stopwords()\n",
    "    paths = ['test.txt', 'train.txt']\n",
    "    clean_vocab(paths)\n",
    "\n",
    "\n",
    "\n",
    "    # load_data(['20news'])\n",
    "    vocab, vocab_dict = load_vocab('vocab.new')\n",
    "    pickle.dump(vocab_dict, open('vocab.pkl', 'wb'))\n",
    "    load_files_nvdm('train.txt', vocab_dict)\n",
    "    load_files_prodlda(paths[0], vocab_dict)\n",
    "\n",
    "    paths = ['test.feat', 'train.feat']\n",
    "    vocab_url=data_dir+'vocab.new'\n",
    "    create_corpus(paths, 'vocab.new')\n",
    "    produce_files(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-partner",
   "metadata": {},
   "source": [
    "### Step2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# import thr necessary libraries \n",
    "import seaborn as sns \n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# to ignore the warnings  \n",
    "from warnings import filterwarnings \n",
    "\n",
    "# Plot for Word Count\n",
    "temp1 = JP_py37[JP_py37[\"WC\"] < 500]\n",
    "\n",
    "sns.set_style('whitegrid') \n",
    "sns.distplot(temp1['WC'], kde = False, color ='red', bins = 100)\n",
    "\n",
    "# Plot for Total Application\n",
    "temp2 = JP_py37[JP_py37[\"total_app\"] < 100]\n",
    "\n",
    "sns.set_style('whitegrid') \n",
    "sns.distplot(temp2['total_app'], kde = False, color ='red', bins = 50)\n",
    "\n",
    "# Plotting Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "sns.set(style='white', font_scale=1.2)\n",
    "\n",
    "# Total Application\n",
    "g = sns.JointGrid(data=JP_py37, x='WC', y='total_app', xlim=(0, 500), ylim=(50, 1000), height=10)\n",
    "g = g.plot_joint(sns.regplot, color=\"xkcd:muted blue\")\n",
    "g = g.plot_marginals(sns.distplot, kde=False, bins=12, color=\"xkcd:bluey grey\")\n",
    "g.ax_joint.text(145, 95, 'r = 0.45, p < .001', fontstyle='italic')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Female Application rate\n",
    "g = sns.JointGrid(data=JP_py37, x='WC', y='women_rate', xlim=(0, 500), ylim=(0, 1), height=10)\n",
    "g = g.plot_joint(sns.regplot, color=\"xkcd:muted blue\")\n",
    "g = g.plot_marginals(sns.distplot, kde=False, bins=12, color=\"xkcd:bluey grey\")\n",
    "g.ax_joint.text(145, 95, 'r = 0.45, p < .001', fontstyle='italic')\n",
    "plt.tight_layout()\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "mpl.rcParams['figure.figsize']=(12.0,12.0)  \n",
    "mpl.rcParams['font.size']=12            \n",
    "mpl.rcParams['savefig.dpi']=100             \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stopwords,\n",
    "                          max_words=1000,\n",
    "                          max_font_size=40, \n",
    "                          random_state=100\n",
    "                         ).generate(str(TM_py37.lemma_body))\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-margin",
   "metadata": {},
   "source": [
    "### Step3. Topic Extraction (Embedded Topic Modeling, ETM) on PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from here when I get back\n",
    "\n",
    "# Working Directory\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\kwonr\\Dropbox\\JobAd_Gender\\Analysis_TopicModeling\\\\Neural_Topic_Models')\n",
    "\n",
    "# Check if GPU is set to work\n",
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from models import ETM\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from dataset import DocDataset\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "#from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "parser = argparse.ArgumentParser('ETM topic model')\n",
    "parser.add_argument('--taskname',type=str,default='cnews10k',help='Taskname e.g cnews10k')\n",
    "parser.add_argument('--no_below',type=int,default=5,help='The lower bound of count for words to keep, e.g 10')\n",
    "parser.add_argument('--no_above',type=float,default=0.005,help='The ratio of upper bound of count for words to keep, e.g 0.3')\n",
    "parser.add_argument('--num_epochs',type=int,default=10,help='Number of iterations (set to 100 as default, but 1000+ is recommended.)')\n",
    "parser.add_argument('--n_topic',type=int,default=20,help='Num of topics')\n",
    "parser.add_argument('--bkpt_continue',type=bool,default=False,help='Whether to load a trained model as initialization and continue training.')\n",
    "parser.add_argument('--use_tfidf',type=bool,default=False,help='Whether to use the tfidf feature for the BOW input')\n",
    "parser.add_argument('--rebuild',type=bool,default=True,help='Whether to rebuild the corpus, such as tokenization, build dict etc.(default True)')\n",
    "parser.add_argument('--batch_size',type=int,default=512,help='Batch size (default=512)')\n",
    "parser.add_argument('--criterion',type=str,default='cross_entropy',help='The criterion to calculate the loss, e.g cross_entropy, bce_softmax, bce_sigmoid')\n",
    "parser.add_argument('--emb_dim',type=int,default=300,help=\"The dimension of the latent topic vectors (default:300)\")\n",
    "parser.add_argument('--auto_adj',action='store_true',help='To adjust the no_above ratio automatically (default:rm top 20)')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    global args\n",
    "    taskname = args.taskname\n",
    "    no_below = args.no_below\n",
    "    no_above = args.no_above\n",
    "    num_epochs = args.num_epochs\n",
    "    n_topic = args.n_topic\n",
    "    n_cpu = cpu_count()-2 if cpu_count()>2 else 2\n",
    "    bkpt_continue = args.bkpt_continue\n",
    "    use_tfidf = args.use_tfidf\n",
    "    rebuild = args.rebuild\n",
    "    batch_size = args.batch_size\n",
    "    criterion = args.criterion\n",
    "    n_topic = args.n_topic\n",
    "    emb_dim = args.emb_dim\n",
    "    auto_adj = args.auto_adj\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    docSet = DocDataset(taskname,no_below=no_below,no_above=no_above,rebuild=rebuild,use_tfidf=False)\n",
    "    if auto_adj:\n",
    "        no_above = docSet.topk_dfs(topk=20)\n",
    "        docSet = DocDataset(taskname,no_below=no_below,no_above=no_above,rebuild=rebuild,use_tfidf=False)\n",
    "    \n",
    "    voc_size = docSet.vocabsize\n",
    "    print('voc size:',voc_size)\n",
    "    model = ETM(bow_dim=voc_size,n_topic=n_topic,taskname=taskname,device=device,emb_dim=emb_dim) #TBD_fc1\n",
    "    model.train(train_data=docSet,batch_size=batch_size,test_data=docSet,num_epochs=num_epochs,log_every=10,beta=1.0,criterion=criterion)\n",
    "    model.evaluate(test_data=docSet)\n",
    "    save_name = f'./ckpt/ETM_{taskname}_tp{n_topic}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}.ckpt'\n",
    "    torch.save(model.vae.state_dict(),save_name)\n",
    "    topic_vecs = model.vae.alpha.weight.detach().cpu().numpy()\n",
    "    word_vecs = model.vae.rho.weight.detach().cpu().numpy()\n",
    "    print('topic_vecs.shape:',topic_vecs.shape)\n",
    "    print('word_vecs.shape:',word_vecs.shape)\n",
    "    vocab = np.array([t[0] for t in sorted(list(docSet.dictionary.token2id.items()),key=lambda x: x[1])]).reshape(-1,1)\n",
    "    topic_ids = np.array([f'TP{i}' for i in range(n_topic)]).reshape(-1,1)\n",
    "    word_vecs = np.concatenate([vocab,word_vecs],axis=1)\n",
    "    topic_vecs = np.concatenate([topic_ids,topic_vecs],axis=1)\n",
    "    #save_name_tp = f'./ckpt/TpVec_ETM_{taskname}_tp{n_topic}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}.emb'\n",
    "    save_name_wd = f'./ckpt/WdVec_ETM_{taskname}_tp{n_topic}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}.emb'\n",
    "    n_instances = word_vecs.shape[0]+topic_vecs.shape[0]\n",
    "    with open(save_name_wd,'w',encoding='utf-8') as wfp:\n",
    "        wfp.write(f'{n_instances} {emb_dim}\\n')\n",
    "        wfp.write('\\n'.join([' '.join(e) for e in word_vecs]+[' '.join(e) for e in topic_vecs]))\n",
    "    from gensim.models import KeyedVectors\n",
    "    w2v = KeyedVectors.load_word2vec_format(save_name_wd,binary=False)\n",
    "    w2v.save(save_name.split('.')[0]+'.w2v')\n",
    "    print(w2v.vocab.keys())\n",
    "    #w2v.most_similar('你好')\n",
    "    for i in range(n_topic):\n",
    "        print(f'Most similar to Topic {i}')\n",
    "        print(w2v.most_similar(f'TP{i}'))\n",
    "    txt_lst, embeds = model.get_embed(train_data=docSet, num=1000)\n",
    "    with open('topic_dist_etm.txt','w',encoding='utf-8') as wfp:\n",
    "        for t,e in zip(txt_lst,embeds):\n",
    "            wfp.write(f'{e}:{t}\\n')\n",
    "    pickle.dump({'txts':txt_lst,'embeds':embeds},open('etm_embeds.pkl','wb'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-louisiana",
   "metadata": {},
   "source": [
    "### Step4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(r'C:\\Users\\kwonr\\Dropbox\\JobAd_Gender\\Analysis_Prediction')\n",
    "\n",
    "# Read pickles\n",
    "Job_Posting = pd.read_pickle(\"Job_Posting.pkl\") \n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Skim\n",
    "Job_Posting.describe()\n",
    "\n",
    "# Normalize topic distribution values \n",
    "bodytopic = ['Body_Topic1', 'Body_Topic2', 'Body_Topic3', 'Body_Topic4', 'Body_Topic5', 'Body_Topic6', 'Body_Topic7', 'Body_Topic8', 'Body_Topic9', 'Body_Topic10', \n",
    "                      'Body_Topic11', 'Body_Topic12', 'Body_Topic13', 'Body_Topic14', 'Body_Topic15', 'Body_Topic16', 'Body_Topic17', 'Body_Topic18', 'Body_Topic19', 'Body_Topic20',\n",
    "                      'Body_Topic21', 'Body_Topic22', 'Body_Topic23', 'Body_Topic24', 'Body_Topic25', 'Body_Topic26', 'Body_Topic27', 'Body_Topic28', 'Body_Topic29', 'Body_Topic30',\n",
    "                      'Body_Topic31', 'Body_Topic32', 'Body_Topic33', 'Body_Topic34', 'Body_Topic35', 'Body_Topic36', 'Body_Topic37', 'Body_Topic38', 'Body_Topic39', 'Body_Topic40',\n",
    "                      'Body_Topic41', 'Body_Topic42', 'Body_Topic43', 'Body_Topic44', 'Body_Topic45', 'Body_Topic46', 'Body_Topic47', 'Body_Topic48']\n",
    "\n",
    "\n",
    "profiletopic = ['Profile_Topic1', 'Profile_Topic2', 'Profile_Topic3', 'Profile_Topic4', 'Profile_Topic5', 'Profile_Topic6', 'Profile_Topic7', 'Profile_Topic8', 'Profile_Topic9', 'Profile_Topic10', \n",
    "                      'Profile_Topic11', 'Profile_Topic12', 'Profile_Topic13', 'Profile_Topic14', 'Profile_Topic15', 'Profile_Topic16', 'Profile_Topic17', 'Profile_Topic18', 'Profile_Topic19', 'Profile_Topic20',\n",
    "                      'Profile_Topic21', 'Profile_Topic22', 'Profile_Topic23', 'Profile_Topic24', 'Profile_Topic25', 'Profile_Topic26', 'Profile_Topic27', 'Profile_Topic28', 'Profile_Topic29', 'Profile_Topic30',\n",
    "                      'Profile_Topic31', 'Profile_Topic32', 'Profile_Topic33', 'Profile_Topic34', 'Profile_Topic35', 'Profile_Topic36', 'Profile_Topic37', 'Profile_Topic38', 'Profile_Topic39', 'Profile_Topic40',\n",
    "                      'Profile_Topic41', 'Profile_Topic42', 'Profile_Topic43', 'Profile_Topic44', 'Profile_Topic45', 'Profile_Topic46']\n",
    "\n",
    "Job_Posting[bodytopic] = Job_Posting[bodytopic].div(Job_Posting.sum_prob_body, axis = 0)\n",
    "Job_Posting['sum_prob_body'] = Job_Posting.loc[:,'Body_Topic1':'Body_Topic48'].sum(axis = 1)\n",
    "\n",
    "Job_Posting[profiletopic] = Job_Posting[profiletopic].div(Job_Posting.sum_prob_profile, axis = 0)\n",
    "Job_Posting['sum_prob_profile'] = Job_Posting.loc[:,'Profile_Topic1':'Profile_Topic46'].sum(axis = 1)\n",
    "\n",
    "#Drop redundant columns\n",
    "Job_Posting.drop(['sum_prob_body', 'sum_prob_profile'], axis=1, inplace=True)\n",
    "\n",
    "# Subsetting (only extract columns needed)\n",
    "import numpy as np\n",
    "loc = Job_Posting.columns.get_loc\n",
    "x = Job_Posting.iloc[:, np.r_[loc('job_role'):loc('Profile_Topic46')+1, loc('women_rate')]]\n",
    "\n",
    "# Check missing values & Drop columns having NA/NaN values\n",
    "x.isnull().sum()\n",
    "x.dropna(inplace = True)\n",
    "\n",
    "y = x['women_rate'].values\n",
    "\n",
    "# Generate a classification threshold\n",
    "print(\"lower 10% is : \", np.quantile(y, .1))\n",
    "print(\"lower 20% is : \", np.quantile(y, .2))\n",
    "print(\"lower 30% is : \", np.quantile(y, .3))\n",
    "print(\"lower 40% is : \", np.quantile(y, .4))\n",
    "print(\"lower 50% is : \", np.quantile(y, .5))\n",
    "print(\"lower 60% is : \", np.quantile(y, .6))\n",
    "print(\"lower 70% is : \", np.quantile(y, .7))\n",
    "print(\"lower 80% is : \", np.quantile(y, .8))\n",
    "print(\"lower 90% is : \", np.quantile(y, .9))\n",
    "\n",
    "y[y >= 0.369942197] = 1\n",
    "y[y < 0.369942197] = 0\n",
    "\n",
    "from collections import Counter\n",
    "B = Counter(y)\n",
    "B\n",
    "# Cleaning \n",
    "\n",
    "# number_of_vacancy\n",
    "x['D_vacancy'] = np.where(x['number_of_vacancy'] >= 10, 1, 0)\n",
    "\n",
    "#job_role & api_job_gender\n",
    "\n",
    "# One-hot encoding \n",
    "x_dummy1 = pd.get_dummies(x.job_role, prefix='job_role')\n",
    "x_dummy2 = pd.get_dummies(x.api_job_gender, prefix='api_job_gender')\n",
    "\n",
    "print(x_dummy1.head())\n",
    "print(x_dummy2.head())\n",
    "\n",
    "# Join the encoded df (later, I should avoid dummy trap)\n",
    "x = x.join(x_dummy1)\n",
    "x = x.join(x_dummy2)\n",
    "\n",
    "# hiring_process\n",
    "\n",
    "x['hiring_process_1'] = np.where(x['hiring_process'].str.contains('1'), 1, 0)\n",
    "x['hiring_process_2'] = np.where(x['hiring_process'].str.contains('2'), 1, 0)\n",
    "x['hiring_process_3'] = np.where(x['hiring_process'].str.contains('3'), 1, 0)\n",
    "x['hiring_process_4'] = np.where(x['hiring_process'].str.contains('4'), 1, 0)\n",
    "x['hiring_process_5'] = np.where(x['hiring_process'].str.contains('5'), 1, 0)\n",
    "\n",
    "# Drop columns as it is now encoded & meaningless columns\n",
    "x = x.drop(['hiring_process', 'number_of_vacancy', 'job_role', 'api_job_gender',\n",
    "            'job_role_12', 'api_job_gender_-1', 'hiring_process_1', 'women_rate'],axis = 1)\n",
    "\n",
    "#Data rype\n",
    "x.info()\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Check the balance of the splits on y_\n",
    "print(np.mean(y_train))\n",
    "print(np.mean(y_test))\n",
    "\n",
    "# Feature Scaling \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# transform data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Cleaning \n",
    "\n",
    "# number_of_vacancy\n",
    "x['D_vacancy'] = np.where(x['number_of_vacancy'] >= 10, 1, 0)\n",
    "\n",
    "#job_role & api_job_gender\n",
    "\n",
    "# One-hot encoding \n",
    "x_dummy1 = pd.get_dummies(x.job_role, prefix='job_role')\n",
    "x_dummy2 = pd.get_dummies(x.api_job_gender, prefix='api_job_gender')\n",
    "\n",
    "print(x_dummy1.head())\n",
    "print(x_dummy2.head())\n",
    "\n",
    "# Join the encoded df (later, I should avoid dummy trap)\n",
    "x = x.join(x_dummy1)\n",
    "x = x.join(x_dummy2)\n",
    "\n",
    "# hiring_process\n",
    "\n",
    "x['hiring_process_1'] = np.where(x['hiring_process'].str.contains('1'), 1, 0)\n",
    "x['hiring_process_2'] = np.where(x['hiring_process'].str.contains('2'), 1, 0)\n",
    "x['hiring_process_3'] = np.where(x['hiring_process'].str.contains('3'), 1, 0)\n",
    "x['hiring_process_4'] = np.where(x['hiring_process'].str.contains('4'), 1, 0)\n",
    "x['hiring_process_5'] = np.where(x['hiring_process'].str.contains('5'), 1, 0)\n",
    "\n",
    "# Drop columns as it is now encoded & meaningless columns\n",
    "x = x.drop(['hiring_process', 'number_of_vacancy', 'job_role', 'api_job_gender',\n",
    "            'job_role_12', 'api_job_gender_-1', 'hiring_process_1', 'women_rate'],axis = 1)\n",
    "\n",
    "#Data rype\n",
    "x.info()\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Check the balance of the splits on y_\n",
    "print(np.mean(y_train))\n",
    "print(np.mean(y_test))\n",
    "\n",
    "# Feature Scaling \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# transform data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = x.columns\n",
    "\n",
    "# Forward\n",
    "from sklearn.linear_model import LinearRegression\n",
    "mlxtend_linear_forward = SFS(LinearRegression(), \n",
    "                             k_features=30, \n",
    "                             forward=True, \n",
    "                             floating=False, \n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             cv=5)\n",
    "\n",
    "mlxtend_linear_forward.fit(x, y, custom_feature_names=feature_names)\n",
    "\n",
    "fig1 = plot_sfs(mlxtend_linear_forward.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. std_err)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Backward\n",
    "mlxtend_linear_backward = SFS(LinearRegression(), \n",
    "                             k_features=30, \n",
    "                             forward=False, \n",
    "                             floating=False, \n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             cv=5)\n",
    "\n",
    "mlxtend_linear_backward.fit(x, y, custom_feature_names=feature_names)\n",
    "\n",
    "fig2 = plot_sfs(mlxtend_linear_backward.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential backward Selection (w. std_err)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Feature Selection - Wrapper Methods\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as sfs\n",
    "\n",
    "# linear regression - forward\n",
    "feature_names = x.columns\n",
    "\n",
    "#for digit in range(5,31):\n",
    "sfs_linear_forward = sfs(LinearRegression(),\n",
    "                         n_features_to_select=digit,\n",
    "                         direction='forward',\n",
    "                         cv=5,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "sfs_linear_forward.fit(x, y)\n",
    "\n",
    "# Print Results - Forward Selection\n",
    "print(\"Features selected by forward sequential selection: \" \n",
    "      f\"{feature_names[sfs_linear_forward.get_support()]}\")\n",
    "\n",
    "print(sfs_linear_forward.transform(x).shape)\n",
    "\n",
    "# linear regression - backward\n",
    "#for digit in range(5,31):\n",
    "feature_names = x.columns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sfs_linear_backward = sfs(LinearRegression(),\n",
    "                         n_features_to_select=30,\n",
    "                         direction='backward',\n",
    "                         cv=5,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "sfs_linear_backward.fit(x, y)\n",
    "\n",
    "# Print Results - Forward Selection\n",
    "print(\"Features selected by backward sequential selection: \" \n",
    "      f\"{feature_names[sfs_linear_backward.get_support()]}\")\n",
    "\n",
    "print(sfs_linear_backward.transform(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-moment",
   "metadata": {},
   "source": [
    "### Step5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "steps = [\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('model', Ridge(alpha=3.8, fit_intercept=True))\n",
    "]\n",
    "\n",
    "ridge_pipe = Pipeline(steps)\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predicting Cross Validation Score the Test set results\n",
    "cv_ridge = cross_val_score(estimator = ridge_pipe, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_ridge_test = ridge_pipe.predict(X_test)\n",
    "\n",
    "# Predicting RMSE the Test set results\n",
    "rmse_ridge = (np.sqrt(mean_squared_error(y_test, y_pred_ridge_test)))\n",
    "print('CV: ', cv_ridge.mean())\n",
    "print(\"RMSE: \", rmse_ridge)\n",
    "\n",
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "steps = [\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('model', Lasso(alpha=0.012, fit_intercept=True, max_iter=3000))\n",
    "]\n",
    "\n",
    "lasso_pipe = Pipeline(steps)\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predicting Cross Validation Score\n",
    "cv_lasso = cross_val_score(estimator = lasso_pipe, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_lasso_test = lasso_pipe.predict(X_test)\n",
    "\n",
    "# Predicting RMSE the Test set results\n",
    "rmse_lasso = (np.sqrt(mean_squared_error(y_test, y_pred_lasso_test)))\n",
    "print('CV: ', cv_lasso.mean())\n",
    "print(\"RMSE: \", rmse_lasso)\n",
    "\n",
    "# Decision Tree\n",
    "#Fitting the Decision Tree Regression Model to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor_dt = DecisionTreeRegressor(random_state = 0)\n",
    "regressor_dt.fit(X_train, y_train)\n",
    "# Predicting Cross Validation Score\n",
    "cv_dt = cross_val_score(estimator = regressor_dt, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_dt_test = regressor_dt.predict(X_test)\n",
    "\n",
    "# Predicting RMSE the Test set results\n",
    "rmse_dt = (np.sqrt(mean_squared_error(y_test, y_pred_dt_test)))\n",
    "print('CV: ', cv_dt.mean())\n",
    "print(\"RMSE: \", rmse_dt)\n",
    "\n",
    "# Fitting the Random Forest Regression to the dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor_rf = RandomForestRegressor(n_estimators = 500, random_state = 0)\n",
    "regressor_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting Cross Validation Score\n",
    "cv_rf = cross_val_score(estimator = regressor_rf, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_rf_test = regressor_rf.predict(X_test)\n",
    "\n",
    "# Predicting RMSE the Test set results\n",
    "rmse_rf = (np.sqrt(mean_squared_error(y_test, y_pred_rf_test)))\n",
    "print('CV: ', cv_rf.mean())\n",
    "print(\"RMSE: \", rmse_rf)\n",
    "\n",
    "models = [('Linear Regression', rmse_linear, cv_linear.mean()),\n",
    "          ('Ridge Regression', rmse_ridge, cv_ridge.mean()),\n",
    "          ('Lasso Regression', rmse_lasso, cv_lasso.mean()),\n",
    "          ('Decision Tree Regression', rmse_dt, cv_dt.mean()),\n",
    "         ]\n",
    "\n",
    "predict = pd.DataFrame(data = models, columns=['Model', 'RMSE', 'Cross-Validation'])\n",
    "predict\n",
    "\n",
    "# Instantiate & Fitting\n",
    "linear_base = LogisticRegression(solver='liblinear')\n",
    "linear_topic = LogisticRegression(solver='liblinear')\n",
    "linear_LIWC = LogisticRegression(solver='liblinear')\n",
    "linear_TLIWC = LogisticRegression(solver='liblinear')\n",
    "linear_LIWCB = LogisticRegression(solver='liblinear')\n",
    "linear_LIWCTopicB = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Fit\n",
    "linear_base.fit(train_base, y_train)\n",
    "linear_topic.fit(train_topic, y_train)\n",
    "linear_LIWC.fit(train_LIWC, y_train)\n",
    "linear_TLIWC.fit(train_TLIWC, y_train)\n",
    "linear_LIWCB.fit(train_LIWCB, y_train)\n",
    "linear_LIWCTopicB.fit(train_LIWCTopicB, y_train)\n",
    "\n",
    "# Predictions on the test dataset\n",
    "y_pred_linear_base = pd.DataFrame(linear_base.predict(test_base))\n",
    "y_pred_linear_topic = pd.DataFrame(linear_topic.predict(test_topic))\n",
    "y_pred_linear_LIWC = pd.DataFrame(linear_LIWC.predict(test_LIWC))\n",
    "y_pred_linear_TLIWC = pd.DataFrame(linear_TLIWC.predict(test_TLIWC))\n",
    "y_pred_linear_LIWCB = pd.DataFrame(linear_LIWCB.predict(test_LIWCB))\n",
    "y_pred_linear_LIWCTopicB = pd.DataFrame(linear_LIWCTopicB.predict(test_LIWCTopicB))\n",
    "\n",
    "# Performance\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_linear_base))\n",
    "print(accuracy_score(y_test, y_pred_linear_topic))\n",
    "print(accuracy_score(y_test, y_pred_linear_LIWC))\n",
    "print(accuracy_score(y_test, y_pred_linear_TLIWC))\n",
    "print(accuracy_score(y_test, y_pred_linear_LIWCB))\n",
    "print(accuracy_score(y_test, y_pred_linear_LIWCTopicB))\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_linear_base))\n",
    "print(metrics.classification_report(y_test, y_pred_linear_topic))\n",
    "print(metrics.classification_report(y_test, y_pred_linear_LIWC))\n",
    "print(metrics.classification_report(y_test, y_pred_linear_TLIWC))\n",
    "print(metrics.classification_report(y_test, y_pred_linear_LIWCB))\n",
    "print(metrics.classification_report(y_test, y_pred_linear_LIWCTopicB))\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    #Predict\n",
    "    predictions = model.predict(X_test)\n",
    "    #probs = pd.DataFrame(model.predict_proba(X_test))\n",
    "    # Store metrics\n",
    "    rf_accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    #rf_roc_auc = metrics.roc_auc_score(y_test, probs[1])\n",
    "    rf_confus_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "    rf_classification_report = metrics.classification_report(y_test, predictions)\n",
    "    rf_precision = metrics.precision_score(y_test, predictions, pos_label=1)\n",
    "    rf_recall = metrics.recall_score(y_test, predictions, pos_label=1)\n",
    "    rf_f1 = metrics.f1_score(y_test, predictions, pos_label=1)\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.4f}%.'.format(rf_accuracy))\n",
    "    #print('rf_roc_auc = {:0.4f}%.'.format(rf_roc_auc))\n",
    "    print(rf_confus_matrix)\n",
    "    print(rf_classification_report)\n",
    "    print('rf_precision = {:0.4f}%.'.format(rf_precision))\n",
    "    print('rf_recall = {:0.4f}%.'.format(rf_recall))\n",
    "    print('rf_f1 = {:0.4f}%.'.format(rf_f1))\n",
    "    return rf_accuracy\n",
    "\n",
    "\n",
    "# FNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "random.seed(1234)\n",
    "\n",
    "#Sigmoid is used as the activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, architecture):\n",
    "        #architecture - numpy array with ith element representing the number of neurons in the ith layer.\n",
    "        \n",
    "        #Initialize the network architecture\n",
    "        self.L = architecture.size - 1 #The index of the last layer L\n",
    "        self.n = architecture #n stores the number of neurons in each layer\n",
    "        self.input_size = self.n[0] #input_size is the number of neurons in the first layer\n",
    "        self.output_size = self.n[self.L] #output_size is the number of neurons in the last layer\n",
    "        \n",
    "        #Parameters will store the network parameters, i.e. the weights and biases\n",
    "        self.parameters = {}\n",
    "        \n",
    "        #Initialize the network weights and biases:\n",
    "        for i in range (1, self.L + 1): \n",
    "            #Initialize weights to small random values\n",
    "            self.parameters['W' + str(i)] = np.random.randn(self.n[i], self.n[i - 1]) * 0.01\n",
    "            \n",
    "            #Initialize rest of the parameters to 1\n",
    "            self.parameters['b' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['z' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['a' + str(i)] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #As we started the loop from 1, we haven't initialized a[0]:\n",
    "        self.parameters['a0'] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #Initialize the cost:\n",
    "        self.parameters['C'] = 1\n",
    "        \n",
    "        #Create a dictionary for storing the derivatives:\n",
    "        self.derivatives = {}\n",
    "        \n",
    "        #Learning rate\n",
    "        self.alpha = 0.01\n",
    "            \n",
    "    def forward_propagate(self, X):\n",
    "        #Note that X here, is just one training example\n",
    "        self.parameters['a0'] = X\n",
    "        \n",
    "        #Calculate the activations for every layer l\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters['z' + str(l)] = np.add(np.dot(self.parameters['W' + str(l)], self.parameters['a' + str(l - 1)]), self.parameters['b' + str(l)])\n",
    "            self.parameters['a' + str(l)] = sigmoid(self.parameters['z' + str(l)])\n",
    "        \n",
    "    def compute_cost(self, y):\n",
    "        self.parameters['C'] = -(y*np.log(self.parameters['a' + str(self.L)]) + (1-y)*np.log( 1 - self.parameters['a' + str(self.L)]))\n",
    "    \n",
    "    def compute_derivatives(self, y):\n",
    "        #Partial derivatives of the cost function with respect to z[L], W[L] and b[L]:        \n",
    "        #dzL\n",
    "        self.derivatives['dz' + str(self.L)] = self.parameters['a' + str(self.L)] - y\n",
    "        #dWL\n",
    "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dz' + str(self.L)], np.transpose(self.parameters['a' + str(self.L - 1)]))\n",
    "        #dbL\n",
    "        self.derivatives['db' + str(self.L)] = self.derivatives['dz' + str(self.L)]\n",
    "\n",
    "        #Partial derivatives of the cost function with respect to z[l], W[l] and b[l]\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            self.derivatives['dz' + str(l)] = np.dot(np.transpose(self.parameters['W' + str(l + 1)]), self.derivatives['dz' + str(l + 1)])*sigmoid_prime(self.parameters['z' + str(l)])\n",
    "            self.derivatives['dW' + str(l)] = np.dot(self.derivatives['dz' + str(l)], np.transpose(self.parameters['a' + str(l - 1)]))\n",
    "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
    "            \n",
    "    def update_parameters(self):\n",
    "        for l in range(1, self.L+1):\n",
    "            self.parameters['W' + str(l)] -= self.alpha*self.derivatives['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= self.alpha*self.derivatives['db' + str(l)]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.forward_propagate(x)\n",
    "    \n",
    "        return self.parameters['a' + str(self.L)]\n",
    "        \n",
    "    def fit(self, X, Y, num_iter):\n",
    "        for iter in range(0, num_iter):\n",
    "            c = 0\n",
    "            acc = 0\n",
    "            n_c = 0\n",
    "            for i in range(0, X.shape[0]):\n",
    "              x = X[i].reshape((X[i].size, 1))\n",
    "              y = Y[i]\n",
    "              self.forward_propagate(x)\n",
    "              self.compute_cost(y)\n",
    "              c += self.parameters['C'] \n",
    "              y_pred = self.predict(x)\n",
    "              y_pred = (y_pred > 0.5)\n",
    "              if y_pred == y:\n",
    "                  n_c += 1\n",
    "              self.compute_derivatives(y)\n",
    "              self.update_parameters()\n",
    "            \n",
    "            c = c/X.shape[0]\n",
    "            acc = (n_c/X.shape[0])*100\n",
    "            print('Iteration: ', iter)\n",
    "            print(\"Cost: \", c)\n",
    "            print(\"Accuracy:\", acc)\n",
    "            \n",
    "#Defining the model architecture\n",
    "architecture = np.array([80, 100, 1])\n",
    "\n",
    "#Creating the classifier\n",
    "classifier = NeuralNetwork(architecture)\n",
    "\n",
    "#Training the classifier\n",
    "classifier.fit(X_train, y_train, 100)\n",
    "\n",
    "#Predicting the test set results:\n",
    "acc = 0\n",
    "n_c = 0\n",
    "for i in range(0, X_test.shape[0]):\n",
    "  x = X_test[i].reshape((X_test[i].size, 1))\n",
    "  y = y_test[i]\n",
    "  y_pred = classifier.predict(x)\n",
    "  y_pred = (y_pred > 0.5)\n",
    "  #print('Expected: %d Got: %d' %(y, y_pred))\n",
    "  if y_pred == y:\n",
    "      n_c += 1\n",
    "\n",
    "acc = (n_c/X_test.shape[0])*100\n",
    "print(\"Test Accuracy\", acc)\n",
    "\n",
    "# First create the base model to tune\n",
    "SVM_classifier = SVC(kernel = 'linear', gamma = 'auto', C=2,probability = True).fit(X_train, y_train)\n",
    "base_accuracy = evaluate(SVM_classifier, X_test, y_test)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "SVM_random = RandomizedSearchCV(estimator = SVM_classifier, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=2, random_state=42, n_jobs = -1)\n",
    "SVM_random.fit(X_train, y_train)\n",
    "\n",
    "best_SVM = SVM_random.best_estimator_\n",
    "SVM_accuracy = evaluate(best_SVM, X_test, y_test)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pprint import pprint\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 1)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,5,10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    #Predict\n",
    "    predictions = model.predict(X_test)\n",
    "    probs = pd.DataFrame(model.predict_proba(X_test))\n",
    "    # Store metrics\n",
    "    rf_accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    rf_roc_auc = metrics.roc_auc_score(y_test, probs[1])\n",
    "    rf_confus_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "    rf_classification_report = metrics.classification_report(y_test, predictions)\n",
    "    rf_precision = metrics.precision_score(y_test, predictions, pos_label=1)\n",
    "    rf_recall = metrics.recall_score(y_test, predictions, pos_label=1)\n",
    "    rf_f1 = metrics.f1_score(y_test, predictions, pos_label=1)\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.4f}%.'.format(rf_accuracy))\n",
    "    print('rf_roc_auc = {:0.4f}%.'.format(rf_roc_auc))\n",
    "    print(rf_confus_matrix)\n",
    "    print(rf_classification_report)\n",
    "    print('rf_precision = {:0.4f}%.'.format(rf_precision))\n",
    "    print('rf_recall = {:0.4f}%.'.format(rf_recall))\n",
    "    print('rf_f1 = {:0.4f}%.'.format(rf_f1))\n",
    "    \n",
    "    return rf_accuracy\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = base_model, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)\n",
    "\n",
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "print(rf_random.best_params_)\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 20, stop = 200, num = 1)]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, y_test)\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances_Topic = list(grid_search.best_estimator_.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "X_TP16_Topic = x\n",
    "Topic_importances = [(feature, round(importances_Topic, 4)) for feature, importances_Topic in zip(X_TP16_Topic, importances_Topic)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "Topic_importances = sorted(Topic_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:80} Importance: {}'.format(*pair)) for pair in Topic_importances];\n",
    "\n",
    "# Cross-Validation for LASSO\n",
    "\n",
    "# Generate a new dataframe (subsetting)\n",
    "LASSO = x[['Analytic', 'b_topic11', 'work', 'affect', 'achieve']]\n",
    "\n",
    "# Renaming for brevity\n",
    "LASSO.rename(columns = {'Analytic':'X1',\n",
    "                       'b_topic11':'X2',\n",
    "                       'work':'X3',\n",
    "                       'affect':'X4',\n",
    "                       'achieve':'X5'}, inplace = True)\n",
    "\n",
    "# Make a superset (squared term = 8)\n",
    "LASSO = LASSO.assign(X1X1 = LASSO.X1 ** 2)\n",
    "LASSO = LASSO.assign(X2X2 = LASSO.X2 ** 2)\n",
    "LASSO = LASSO.assign(X3X3 = LASSO.X3 ** 2)\n",
    "LASSO = LASSO.assign(X4X4 = LASSO.X4 ** 2)\n",
    "LASSO = LASSO.assign(X5X5 = LASSO.X5 ** 2)\n",
    "#LASSO = LASSO.assign(X6X6 = LASSO.X6 ** 2)\n",
    "#LASSO = LASSO.assign(X7X7 = LASSO.X7 ** 2)\n",
    "#LASSO = LASSO.assign(X8X8 = LASSO.X8 ** 2)\n",
    "\n",
    "# Make a superset (interaction term = 8C2)\n",
    "LASSO = LASSO.assign(X1X2 = LASSO.X1 * LASSO.X2)\n",
    "LASSO = LASSO.assign(X1X3 = LASSO.X1 * LASSO.X3)\n",
    "LASSO = LASSO.assign(X1X4 = LASSO.X1 * LASSO.X4)\n",
    "LASSO = LASSO.assign(X1X5 = LASSO.X1 * LASSO.X5)\n",
    "#LASSO = LASSO.assign(X1X6 = LASSO.X1 * LASSO.X6)\n",
    "#LASSO = LASSO.assign(X1X7 = LASSO.X1 * LASSO.X7)\n",
    "#LASSO = LASSO.assign(X1X8 = LASSO.X1 * LASSO.X8)\n",
    "LASSO = LASSO.assign(X2X3 = LASSO.X2 * LASSO.X3)\n",
    "LASSO = LASSO.assign(X2X4 = LASSO.X2 * LASSO.X4)\n",
    "LASSO = LASSO.assign(X2X5 = LASSO.X2 * LASSO.X5)\n",
    "#LASSO = LASSO.assign(X2X6 = LASSO.X2 * LASSO.X6)\n",
    "#LASSO = LASSO.assign(X2X7 = LASSO.X2 * LASSO.X7)\n",
    "#LASSO = LASSO.assign(X2X8 = LASSO.X2 * LASSO.X8)\n",
    "LASSO = LASSO.assign(X3X4 = LASSO.X3 * LASSO.X4)\n",
    "LASSO = LASSO.assign(X3X5 = LASSO.X3 * LASSO.X5)\n",
    "#LASSO = LASSO.assign(X3X6 = LASSO.X3 * LASSO.X6)\n",
    "#LASSO = LASSO.assign(X3X7 = LASSO.X3 * LASSO.X7)\n",
    "#LASSO = LASSO.assign(X3X8 = LASSO.X3 * LASSO.X8)\n",
    "LASSO = LASSO.assign(X4X5 = LASSO.X4 * LASSO.X5)\n",
    "#LASSO = LASSO.assign(X4X6 = LASSO.X4 * LASSO.X6)\n",
    "#LASSO = LASSO.assign(X4X7 = LASSO.X4 * LASSO.X7)\n",
    "#LASSO = LASSO.assign(X4X8 = LASSO.X4 * LASSO.X8)\n",
    "#LASSO = LASSO.assign(X5X6 = LASSO.X5 * LASSO.X6)\n",
    "#LASSO = LASSO.assign(X5X7 = LASSO.X5 * LASSO.X7)\n",
    "#LASSO = LASSO.assign(X5X8 = LASSO.X5 * LASSO.X8)\n",
    "#LASSO = LASSO.assign(X6X7 = LASSO.X6 * LASSO.X7)\n",
    "#LASSO = LASSO.assign(X6X8 = LASSO.X6 * LASSO.X8)\n",
    "#LASSO = LASSO.assign(X7X8 = LASSO.X7 * LASSO.X8)\n",
    "\n",
    "# Cross-validation for LASSO Hyperparameter Selection\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "# define model\n",
    "alphas = np.logspace(-12, 1, 400)\n",
    "LASSO_model = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "# fit model\n",
    "LASSO_model.fit(X_train, y_train)\n",
    "# summarize chosen configuration\n",
    "print('alpha: %f' % LASSO_model.alpha_) # alpha: 0.003028\n",
    "\n",
    "# Visualization\n",
    "from yellowbrick.datasets import load_concrete\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "visualizer = AlphaSelection(LASSO_model)\n",
    "visualizer.fit(LASSO, y)\n",
    "visualizer.show()\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "# Sub -sample\n",
    "# Sample Splitting \n",
    "LASSO1 = LASSO.iloc[0:1000, :]\n",
    "LASSO2 = LASSO.iloc[1000:2000, :]\n",
    "LASSO3 = LASSO.iloc[2000:3000, :]\n",
    "LASSO4 = LASSO.iloc[3000:4000, :]\n",
    "LASSO5 = LASSO.iloc[4000:5000, :]\n",
    "LASSO6 = LASSO.iloc[5000:6000, :]\n",
    "LASSO7 = LASSO.iloc[6000:7000, :]\n",
    "LASSO8 = LASSO.iloc[7000:8000, :]\n",
    "LASSO9 = LASSO.iloc[8000:9000, :]\n",
    "LASSO10 = LASSO.iloc[9000:10358, :]\n",
    "\n",
    "print(len(LASSO1))\n",
    "print(len(LASSO2))\n",
    "print(len(LASSO3))\n",
    "print(len(LASSO4))\n",
    "print(len(LASSO5))\n",
    "print(len(LASSO6))\n",
    "print(len(LASSO7))\n",
    "print(len(LASSO8))\n",
    "print(len(LASSO9))\n",
    "print(len(LASSO10))\n",
    "\n",
    "y1 = y[0:1000]\n",
    "y2 = y[1000:2000]\n",
    "y3 = y[2000:3000]\n",
    "y4 = y[3000:4000]\n",
    "y5 = y[4000:5000]\n",
    "y6 = y[5000:6000]\n",
    "y7 = y[6000:7000]\n",
    "y8 = y[7000:8000]\n",
    "y9 = y[8000:9000]\n",
    "y10 = y[9000:10358]\n",
    "\n",
    "print(len(y1))\n",
    "print(len(y2))\n",
    "print(len(y3))\n",
    "print(len(y4))\n",
    "print(len(y5))\n",
    "print(len(y6))\n",
    "print(len(y7))\n",
    "print(len(y8))\n",
    "print(len(y9))\n",
    "print(len(y10))\n",
    "\n",
    "# Full sample\n",
    "# define model\n",
    "alphas = np.logspace(-12, 1, 400)\n",
    "LASSO_model1 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model2 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model3 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model4 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model5 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model6 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model7 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model8 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model9 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "LASSO_model10 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "LASSO_model1.fit(LASSO1, y1)\n",
    "LASSO_model2.fit(LASSO2, y2)\n",
    "LASSO_model3.fit(LASSO3, y3)\n",
    "LASSO_model4.fit(LASSO4, y4)\n",
    "LASSO_model5.fit(LASSO5, y5)\n",
    "LASSO_model6.fit(LASSO6, y6)\n",
    "LASSO_model7.fit(LASSO7, y7)\n",
    "LASSO_model8.fit(LASSO8, y8)\n",
    "LASSO_model9.fit(LASSO9, y9)\n",
    "LASSO_model10.fit(LASSO10, y10)\n",
    "\n",
    "#Save coefficients\n",
    "lasso_coeff = pd.DataFrame()\n",
    "lasso_coeff['Full sample'] = pd.Series(LASSO_model.coef_)\n",
    "lasso_coeff['subsample1'] = pd.Series(LASSO_model1.coef_)\n",
    "lasso_coeff['subsample2'] = pd.Series(LASSO_model2.coef_)\n",
    "lasso_coeff['subsample3'] = pd.Series(LASSO_model3.coef_)\n",
    "lasso_coeff['subsample4'] = pd.Series(LASSO_model4.coef_)\n",
    "lasso_coeff['subsample5'] = pd.Series(LASSO_model5.coef_)\n",
    "lasso_coeff['subsample6'] = pd.Series(LASSO_model6.coef_)\n",
    "lasso_coeff['subsample7'] = pd.Series(LASSO_model7.coef_)\n",
    "lasso_coeff['subsample8'] = pd.Series(LASSO_model8.coef_)\n",
    "lasso_coeff['subsample9'] = pd.Series(LASSO_model9.coef_)\n",
    "lasso_coeff['subsample10'] = pd.Series(LASSO_model10.coef_)\n",
    "lasso_coeff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(lasso_coeff, \n",
    "                 linewidths=.5, \n",
    "                 robust=True ,\n",
    "                 annot_kws = {'size':14}, \n",
    "                 yticklabels=LASSO.columns, \n",
    "                 center=0, \n",
    "                 cmap='coolwarm')\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.figure.set_size_inches((12, 10))\n",
    "\n",
    "#Bootstrap samples\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#organizing dataset \n",
    "#add 'y' array as new column in DataFrame\n",
    "values = LASSO\n",
    "values['y'] = y.tolist()\n",
    "\n",
    "# Bootstrap configuration\n",
    "n_iterations = 30  #Nso. of bootstrap samples to be repeated (created)\n",
    "n_size = int(len(values) * 0.80) #Size of sample, picking only 80% of the given data in every bootstrap sample\n",
    "\n",
    "#Lets run Bootstrap\n",
    "lasso_boostrap0 = pd.DataFrame()\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    #prepare train & test sets\n",
    "    train = resample(values, n_samples = n_size) #Sampling with replacement..whichever is not used in training data will be used in test data\n",
    "    test = np.array([x for x in values if x not in train]) #picking rest of the data not considered in training sample\n",
    "    \n",
    "    # define model evaluation method\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "    # define model\n",
    "    alphas = np.logspace(-12, 1, 400)\n",
    "    LASSO_model0 = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
    "    # fit model\n",
    "    LASSO_model0.fit(train.iloc[:,:-1], train.iloc[:,-1])\n",
    "    #coefficient\n",
    "    result0 = pd.DataFrame(LASSO_model0.coef_)\n",
    "    lasso_boostrap0 = pd.concat([lasso_boostrap0, result0], axis=1)\n",
    "    \n",
    "lasso_boostrap0['mean_estimate'] = lasso_boostrap0.mean(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
